import pandas as pd
import numpy as np
import midterm1_helper as mh
import matplotlib.pyplot as plt
import re



hf_series = pd.read_excel('data/proshares_analysis_data.xlsx',sheet_name='hedge_fund_series')
merrill_factors = pd.read_excel('data/proshares_analysis_data.xlsx',sheet_name='merrill_factors',index_col=0)
other_data = pd.read_excel('data/proshares_analysis_data.xlsx',sheet_name='other_data',index_col=0)


hf_series.head()


# Set Index - Date
hf_series = hf_series.rename(columns={'Unnamed: 0':'Date'})
hf_series.set_index('Date', inplace=True)
hf_series.head()


mh.return_metrics(hf_series, portfolio=None, annual_factor=12)


mh.risk_metrics(hf_series, portfolio=None, quantile=0.05, relative=False, mdd=True)


mh.get_ols_metrics(regressors=merrill_factors['SPY US Equity'], targets=hf_series, annualization=12, ignorenan=True, intercept=True).T


mh.display_correlation(df=hf_series,annot=True,list_maxmin=True)


# Static Insample with Intercept
mh.get_ols_metrics(regressors=merrill_factors, targets=hf_series['HFRIFWI Index'], annualization=12, ignorenan=True, intercept=True).T


# Static Insample without Intercept
mh.get_ols_metrics(regressors=merrill_factors, targets=hf_series['HFRIFWI Index'], annualization=12, ignorenan=True, intercept=False).T


rolling_betas_with_int, rolling_betas_without_int, replication_results, performance_metrics = mh.get_rolling_ols_metrics(
    regressors=merrill_factors,
    targets=hf_series['HFRIFWI Index'],
    window=60,
    annualization=12,
    intercept=True
)


rolling_betas_with_int.tail()


rolling_betas_without_int.tail()


replication_results.tail()


metrics_df = pd.DataFrame(performance_metrics, index=[0]).T
metrics_df.columns = ['Value']

# Manually create 'Method' and 'Metric' columns
metrics_df['Method'] = ['IS_Int', 'IS_Int', 'IS_Int',
                        'OOS_Int', 'OOS_Int', 'OOS_Int',
                        'IS_NoInt', 'IS_NoInt', 'IS_NoInt',
                        'OOS_NoInt', 'OOS_NoInt', 'OOS_NoInt',
                        'Static_IS_Int', 'Static_IS_Int', 'Static_IS_Int',
                        'Static_IS_NoInt', 'Static_IS_NoInt', 'Static_IS_NoInt']

metrics_df['Metric'] = ['R²', 'Corr', 'MeanError',
                        'R²', 'Corr', 'MeanError',
                        'R²', 'Corr', 'MeanError',
                        'R²', 'Corr', 'MeanError',
                        'R²', 'Corr', 'MeanError',
                        'R²', 'Corr', 'MeanError']

clean_df = metrics_df.pivot(index='Metric', columns='Method', values='Value')
clean_df



round(replication_results['Rolling-IS-Int'].mean(),4)


round(replication_results['Rolling-IS-NoInt'].mean(), 4)


round(replication_results['HFRIFWI Index'].mean(), 4)


round(replication_results['Static-IS-Int'].mean(),4)


round(replication_results['Static-IS-NoInt'].mean(),4)


mh.get_nnls_estimates(regressors=merrill_factors, targets=hf_series['HFRIFWI Index']).T


bounds = [(0, 1)] * len(merrill_factors.columns)  # Example: constrain all betas between 0 and 1
mh.get_glm_estimates_with_constraints(regressors=merrill_factors, targets=hf_series['HFRIFWI Index'], bounds=bounds).T


mh.get_ols_metrics(regressors=merrill_factors, targets=other_data['HEFA US Equity'], annualization=12, ignorenan=True, intercept=True).T


mh.get_ols_metrics(
    regressors=merrill_factors, 
    targets=other_data['TRVCI Index'], 
    annualization=12, 
    ignorenan=True, 
    intercept=True
).T


spy_and_tail = pd.concat([other_data[['TAIL US Equity']], merrill_factors['SPY US Equity']], axis=1).dropna()
mh.return_metrics(spy_and_tail)


spy_and_tail = pd.concat([other_data[['TAIL US Equity']], merrill_factors['SPY US Equity']], axis=1).dropna()
mh.risk_metrics(spy_and_tail).T


mh.get_ols_metrics(
    regressors=merrill_factors, 
    targets=other_data['TAIL US Equity'], 
    annualization=12, 
    ignorenan=True, 
    intercept=True
).T


spy_and_tail.corr()


data = pd.read_excel('data/spy_data.xlsx', sheet_name='total returns')
data['date'] = pd.to_datetime(data['date'])
data['SPY Excess'] = data['SPY'] - data['^IRX']
df = data.copy().drop(columns=['SPY', '^IRX']).rename(columns={'date':'Date'}).set_index('Date')
df.head()


historic_expanding_var_df, historic_expanding_under_var_frequency = mh.calculate_historic_expanding_var(
    data = df, 
    start_date='2001-01-02',
    quantile=0.05, 
    min_window_size=60
)   
print(historic_expanding_under_var_frequency)
historic_expanding_var_df.loc['2001-01-02':].head()


historic_rolling_var_df, historic_rolling_under_var_frequency = mh.calculate_historic_rolling_var(
    data = df, 
    start_date='2001-01-02',
    quantile=0.05, 
    window_size=60
)   
print(historic_rolling_under_var_frequency)
historic_rolling_var_df.loc['2001-01-02':].head()


hit_ratio_error = historic_rolling_under_var_frequency['SPY Excess']/0.05-1
abs(hit_ratio_error)


volatility_VaR = mh.calculate_volatility_var(
    data=df,
    ewma_theta=0.94,
    ewma_initial_vol=.2/np.sqrt(252),
    window_size=252, 
    z_score=-1.65, 
    start_date='2001-01-02'
)


volatility_VaR.tail()


num_violations, hit_ratio, hit_error = mh.calculate_var_violations(
    var_series = historic_expanding_var_df['VaR_SPY Excess'].loc['2001-01-02':], 
    excess_returns = df['SPY Excess'].loc['2001-01-02':], 
    expected_quantile = 0.05)
print(num_violations, hit_ratio, hit_error )


from cmds.portfolio_management_helper import calc_var_cvar_summary


spy_volatility = calc_var_cvar_summary(
    df,
    window=252,
    quantile=.05,
    ewma_theta=.94,
    ewma_initial_vol=.2/np.sqrt(252),
    z_score=-1.65,
    keep_columns=["Volatility"],
    drop_columns=["GARCH"]
)
spy_volatility.plot(title="Volatility of SPY (Alternative EMWA Theta)", xlabel="Date", ylabel="Volatility", alpha=.8)


spy_volatility


spy_parametric_var = calc_var_cvar_summary(
    df,
    window=252,
    quantile=.05,
    ewma_theta=.94,
    ewma_initial_vol=.2/np.sqrt(252),
    z_score=-1.65,
    keep_columns=["Expanding 252 Parametric VaR", "Rolling 252 Parametric VaR", "EWMA 0.94 Parametric VaR"]
)
spy_parametric_var.tail(3)


(
    spy_parametric_var
    .dropna()
    .plot(title="Parametric VaR", xlabel="Date", ylabel="Returns")
)


calc_var_cvar_summary(
    df,
    window=252,
    quantile=.05,
    ewma_theta=.94,
    ewma_initial_vol=.2/np.sqrt(252),
    filter_first_hit_ratio_date="2001-01-01",
    z_score=-1.65,
    return_hit_ratio=True,
    shift=1,
    drop_indexes=["GARCH"]
)


spy_cvar = pd.concat([
    mh.historical_expanding_cvar(df["SPY Excess"].shift()).to_frame("Expanding CVaR"),
    mh.historical_rolling_cvar(df["SPY Excess"].shift()).to_frame("Rolling CVaR")
], axis=1)
spy_cvar.tail(5)


spy_var_cvar_stats = calc_var_cvar_summary(
    df,
    window=252,
    ewma_theta = 0.95,
    drop_columns=["GARCH"],
    z_score=-1.65
)
spy_var_cvar_stats
# spy_cvar = (
#     df
#     .loc[:, lambda df: df.columns[df.columns.map(lambda x: bool(re.search("CVaR", x)))]]
#      .loc["2001-01-01":]
# )
# spy_cvar.tail()








import cmds.portfolio_management_helper as pmh


assets_excess_returns = pd.read_excel("data/multi_asset_etf_data.xlsx", sheet_name="excess returns").set_index('Date')
assets_excess_returns.tail()


pmh.calc_summary_statistics(
    assets_excess_returns,
    annual_factor=12,
    provided_excess_returns=True,
    keep_columns=['Annualized Vol', 'Annualized Mean', 'Annualized Sharpe']
)


(
    pmh.calc_summary_statistics(
        assets_excess_returns,
        annual_factor=12,
        provided_excess_returns=True,
        keep_columns=['Annualized Sharpe']
    )
    .sort_values("Annualized Sharpe")
    .reset_index()
    .loc[lambda df: df.index.isin([0, len(df.index)-1])]
    .rename(columns={"index": "Asset"})
    .assign(Label=["Worst Sharpe", "Best Sharpe"])
)


pmh.calc_correlations(assets_excess_returns)


pmh.calc_cummulative_returns(assets_excess_returns[['TIP', 'BWX', 'IEF']])


pmh.calc_summary_statistics(
    assets_excess_returns[['TIP', 'BWX', 'IEF']],
    annual_factor=12,
    provided_excess_returns=True,
    keep_columns=[
        "Annualized Mean", "Annualized Vol", "Annualized Sharpe"
    ]
).transpose()


analysis_sharpe_vs_tangent_weights = (
    pmh.calc_summary_statistics(
        assets_excess_returns,
        annual_factor=12,
        provided_excess_returns=True,
        keep_columns=['Tangency Weights', 'Annualized Sharpe']
    )
    .sort_values('Annualized Sharpe', ascending=False)
)
analysis_sharpe_vs_tangent_weights


pmh.calc_tangency_weights(assets_excess_returns,return_graphic=True)


pmh.calc_summary_statistics(
    pmh.calc_tangency_weights(assets_excess_returns, name="Tangency", return_port_ret=True),
    annual_factor=12,
    provided_excess_returns=True,
    keep_columns=['Annualized Sharpe', 'Annualized Mean', 'Annualized Vol']
)


TIP_ADJUSTMENT = 0.0012

# Modified TIPS
assets_excess_returns_modified_tips = (
    assets_excess_returns
    .assign(TIP=lambda df: df.TIP + TIP_ADJUSTMENT)
)

# No TIPS
assets_excess_returns_no_tips = assets_excess_returns.drop("TIP", axis=1)

pd.concat([
    pmh.calc_tangency_weights(assets_excess_returns_no_tips, name="No TIPS Tangency"),
    pmh.calc_tangency_weights(assets_excess_returns_modified_tips, name="Mod TIPS Tangency"),
    pmh.calc_tangency_weights(assets_excess_returns)
], axis=1)


analysis_tips_portfolios = pd.concat([
    pmh.calc_tangency_weights(assets_excess_returns_modified_tips, name="Mod TIPS Tangency", return_port_ret=True),
    pmh.calc_tangency_weights(assets_excess_returns_no_tips, name="No TIPS Tangency", return_port_ret=True),
    pmh.calc_tangency_weights(assets_excess_returns, return_port_ret=True)
], axis=1)

pmh.calc_summary_statistics(
    analysis_tips_portfolios,
    annual_factor=12,
    provided_excess_returns=True,
    keep_columns=["Annualized Mean", "Annualized Vol", "Annualized Sharpe"]
)


n_assets = len(assets_excess_returns.columns)
MU_MONTH_TARGET = 0.010

portfolio_equal_weights_not_scaled = pmh.create_portfolio(
    assets_excess_returns,
    weights=[1 / n_assets for _ in range(n_assets)],
    port_name="Equal Weights"
)
portfolio_equal_weights = portfolio_equal_weights_not_scaled * MU_MONTH_TARGET / portfolio_equal_weights_not_scaled.mean()
portfolio_equal_weights


asset_variance_dict = assets_excess_returns.std().map(lambda x: x ** 2).to_dict()
asset_inv_variance_dict = {asset: 1 / variance for asset, variance in asset_variance_dict.items()}
portfolio_risk_parity_not_scaled = pmh.create_portfolio(
    assets_excess_returns,
    weights=asset_inv_variance_dict,
    port_name="Risk Parity"
)
portfolio_risk_parity = portfolio_risk_parity_not_scaled * MU_MONTH_TARGET / portfolio_risk_parity_not_scaled.mean()
portfolio_risk_parity


portfolio_tangency_not_scaled = pmh.calc_tangency_weights(assets_excess_returns, return_port_ret=True)
portfolio_tangency = portfolio_tangency_not_scaled * MU_MONTH_TARGET / portfolio_tangency_not_scaled.mean()
portfolio_tangency


portfolio_regularized_not_scaled = pmh.calc_tangency_weights(assets_excess_returns, return_port_ret=True, cov_mat=.5, name="Regularized")
portfolio_regularized = portfolio_regularized_not_scaled * MU_MONTH_TARGET / portfolio_regularized_not_scaled.mean()
portfolio_regularized


portfolios = pd.concat([
    portfolio_equal_weights,
    portfolio_risk_parity,
    portfolio_regularized,
    portfolio_tangency
], axis=1)
pmh.calc_summary_statistics(
    portfolios,
    provided_excess_returns=True,
    annual_factor=12,
    keep_columns=['Annualized Mean', 'Annualized Vol', 'Annualized Sharpe']
)



