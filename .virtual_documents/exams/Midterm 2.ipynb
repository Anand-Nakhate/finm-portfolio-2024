











import pandas as pd
import numpy as np
from tabulate import tabulate
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
import scipy.stats as stats
from scipy.stats import norm



df_sector = pd.read_excel('../data/midterm_2_data.xlsx', sheet_name='sector excess returns').set_index('date')
df_factors = pd.read_excel('../data/midterm_2_data.xlsx', sheet_name='factors excess returns').set_index('date')


df_sector.head()


df_factors.head()


















































train_start, train_end = '2018-01-01', '2022-12-01'
test_start, test_end = '2023-01-01', min(df_factors.index.max(), df_sector.index.max())
df_sector_train = df_sector.loc[train_start:train_end]
df_factors_train = df_factors.loc[train_start:train_end]
df_sector_test = df_sector.loc[test_start:test_end]
df_factors_test = df_factors.loc[test_start:test_end]





# Note: The dataset has an additional space after Food and Soda in column names
factor_premia = df_factors_train.mean()

expected_excess_returns = {}
betas = {}

for asset in df_sector_train.columns:
    X = df_factors_train[['MKT', 'HML', 'RMW', 'UMD']]
    y = df_sector_train[asset]

    model = sm.OLS(y, X).fit()

    betas[asset] = model.params

    lambda_i = model.params @ factor_premia.values
    expected_excess_returns[asset] = lambda_i

for asset in ['Agric', 'Food ', 'Soda ']:
    print(f"{asset}: \nλ = {expected_excess_returns[asset]}, \nβ = {betas[asset]}\n")





# Note: Using the training sample for the calculations.
lambda_vector = np.array(list(expected_excess_returns.values()))

cov_matrix = df_sector_train.cov()
diag_cov = np.diag(np.diag(cov_matrix)) 
reg_cov = (cov_matrix + diag_cov) / 2  
inv_reg_cov = np.linalg.inv(reg_cov)

ones = np.ones(len(lambda_vector))
scaling_factor_reg = 1 / (ones.T @ inv_reg_cov @ lambda_vector)
reg_weights = scaling_factor_reg * (inv_reg_cov @ lambda_vector)

assets = df_sector_train.columns
weights_dict = dict(zip(assets, reg_weights))

for asset in ['Agric', 'Food ', 'Soda ']:
    print(f"{asset}: {weights_dict[asset]}")





test_returns = df_sector_test @ reg_weights  
def calculate_univariate_statistics(df, annual_factor=12):
    if isinstance(df, pd.Series):
        df = df.to_frame()

    mean_return = df.mean() * annual_factor
    volatility = df.std() * np.sqrt(annual_factor)
    sharpe_ratio = mean_return / volatility

    stats_df = pd.DataFrame({
        'mean': mean_return,
        'vol': volatility,
        'sharpe': sharpe_ratio
    })

    return stats_df.T
calculate_univariate_statistics(test_returns)





sample_excess_returns = df_sector.loc[train_start:train_end].mean()

cov_matrix = df_sector.loc[train_start:train_end].cov()
diag_cov = np.diag(np.diag(cov_matrix))
reg_cov = (cov_matrix + diag_cov) / 2
inv_reg_cov = np.linalg.inv(reg_cov)

ones = np.ones(len(sample_excess_returns))
scaling_factor = 1 / (ones.T @ inv_reg_cov @ sample_excess_returns.values)
reg_weights = scaling_factor * (inv_reg_cov @ sample_excess_returns.values)

assets = df_sector.columns
weights_dict = dict(zip(assets, reg_weights))

for asset in ['Agric', 'Food ', 'Soda ']:
    print(f"{asset}: {weights_dict[asset]}")


df_sector_test = df_sector.loc[test_start:test_end]
portfolio_returns = df_sector_test @ reg_weights

calculate_univariate_statistics(portfolio_returns)











cov_matrix_factors = df_factors_train.cov()
diag_cov_factors = np.diag(np.diag(cov_matrix_factors))
reg_cov_factors = (cov_matrix_factors + diag_cov_factors) / 2
inv_reg_cov_factors = np.linalg.inv(reg_cov_factors)

ones = np.ones(len(factor_premia))
scaling_factor = 1 / (ones.T @ inv_reg_cov_factors @ factor_premia)
factor_weights = scaling_factor * (inv_reg_cov_factors @ factor_premia)

test_start, test_end = '2023-01-01', '2023-12-01'
portfolio_returns = df_factors_test @ factor_weights

calculate_univariate_statistics(portfolio_returns, 12)

















# 3.1
log_returns = np.log1p(df_factors)
log_returns.head()


summary_annualized = calculate_univariate_statistics(log_returns, 12)
summary_annualized


# 3.2
# Note: 15 yrs interpretation - 15*Annual. Instead of using the most recent/ starting 15 years worth of data.
annual_mean = summary_annualized.loc['mean']
annual_vol = summary_annualized.loc['vol']

mean_15yr = 15 * annual_mean
vol_15yr = np.sqrt(15) * annual_vol
sharpe_15yr = mean_15yr / vol_15yr

summary_cum15_corrected = pd.DataFrame({
    'mean': mean_15yr,
    'vol': vol_15yr,
    'sharpe': sharpe_15yr
})
summary_cum15_corrected
# Reference GPT Prompt: What does it exactly mean by "15-year cumulative log excess returns"? How do I calculate it?








def prob(mu, sigma, h):
    return norm.cdf(-np.sqrt(h) * mu / sigma)

mu_monthly = annual_mean['UMD'] / 12
sigma_monthly = annual_vol['UMD'] / np.sqrt(12)
prob_single = norm.cdf(-mu_monthly / sigma_monthly)

mu_15yr = 15 * annual_mean['UMD']
sigma_15yr = np.sqrt(15) * annual_vol['UMD']
prob_15yr = norm.cdf(-mu_15yr / sigma_15yr)

print(f"Probability of negative return (single period): {prob_single}")
print(f"Probability of negative return (15-year period): {prob_15yr}")





log_returns_2009 = log_returns.loc['2009-01-01':]

annual_mean_2009 = log_returns_2009.mean() * 12
annual_vol_2009 = log_returns_2009.std() * np.sqrt(12)

mu_diff_monthly = (annual_mean_2009['UMD'] - annual_mean_2009['MKT']) / 12
sigma_diff_monthly = np.sqrt((annual_vol_2009['UMD']**2 + annual_vol_2009['MKT']**2) / 12)
prob_single = norm.cdf(mu_diff_monthly / sigma_diff_monthly)

mu_diff_15yr = 15 * (annual_mean_2009['UMD'] - annual_mean_2009['MKT'])
sigma_diff_15yr = np.sqrt(15) * np.sqrt(annual_vol_2009['UMD']**2 + annual_vol_2009['MKT']**2)
prob_15yr = norm.cdf(mu_diff_15yr / sigma_diff_15yr)

print(f"Probability of UMD outperforming MKT (single period): {prob_single}")
print(f"Probability of UMD outperforming MKT (15-year period): {prob_15yr}")
# Reference: HW 6 4.4





















