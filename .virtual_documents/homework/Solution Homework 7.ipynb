








import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.regression.rolling import RollingOLS

import warnings
warnings.filterwarnings("ignore")























rets = pd.read_excel('gmo_data.xlsx',sheet_name='total returns',index_col='date')
rfr = pd.read_excel('gmo_data.xlsx',sheet_name='risk-free rate',index_col='date') / 12
retsx = rets.subtract(rfr['TBill 3M'],axis=0)


# Create function to retrieve common statistics
def stats_mean_vol_sharpe(data,portfolio = None,portfolio_name = 'Portfolio',annualize = 12):
    
    if portfolio is None:
        returns = data
    else:
        returns = data @ portfolio
    
    output = returns.agg(['mean','std'])
    output.loc['sharpe'] = output.loc['mean'] / output.loc['std']
    
    output.loc['mean'] *= annualize
    output.loc['std'] *= np.sqrt(annualize)
    output.loc['sharpe'] *= np.sqrt(annualize)
    
    if portfolio is None:
        pass
    else:
        output.columns = [portfolio_name]
    
    return output

# Create function to calculate stats over certain periods, using any stat function
def stats_over_samples(data, samples, statfunc, **kwargs):
    '''enter (start,end) in samples as a list of tuples'''
    dfs = []
    output = pd.DataFrame()
    
    for start, end in samples:
        
        if start == 'start':
            
            if end == 'end':
                data_subsample = data
            else:
                data_subsample = data.loc[:end]
        else:
            
            if end == 'end':
                data_subsample = data.loc[start:]
            else:
                data_subsample = data.loc[start:end]
        
        stats = statfunc(data_subsample,**kwargs)
        dfs.append(stats)
        
    output = pd.concat(dfs,keys = [f'{start} - {end}' for start,end in samples])
    
    return output


# Create function to retrieve tail risk statistics
# Note this uses a simple historic definition of VaR and CVaR
def stats_tail_risk(data, portfolio = None, portfolio_name = 'Portfolio', VaR = 0.05):
    
    if portfolio is None:
        returns = data
    else:
        returns = data @ portfolio
    
    output = returns.agg(['skew',
                          'kurt'])
    output.loc['VaR'] = returns.quantile(q = 0.05)
    output.loc['CVaR'] = returns[returns <= output.loc['VaR']].mean()
    output = output.append(max_drawdown(returns,portfolio,portfolio_name))
    ### ^^^ THIS STEP ONLY NECESSARY IF LOOKING FOR MAX DRAWDOWN AS WELL
    if portfolio is None:
        pass
    else:
        output.columns = portfolio_name
    
    return output


# Create function to calculate max drawdown and associated dates
def max_drawdown(data, portfolio = None, portfolio_name = 'Portfolio'):
    
    if portfolio is None:
        returns = data
        output = pd.DataFrame(columns=returns.columns)
    else:
        returns = data @ portfolio
        output = pd.DataFrame(columns=[portfolio_name])
    
    cumulative = (returns + 1).cumprod()
    maximum = cumulative.expanding().max()
    drawdown = cumulative / maximum - 1
    
    for col in output.columns:
        
        output.loc['MDD',col] = drawdown[col].min()
        output.loc['Max Date',col] = cumulative[cumulative.index < drawdown[col].idxmin()][col]\
                                             .idxmax()\
                                             .date()
        output.loc['Min Date',col] = drawdown[col].idxmin().date()
        recovery_date = drawdown.loc[drawdown[col].idxmin():,col]\
                                             .apply(lambda x: 0 if x == 0 else np.nan)\
                                             .idxmax()
        
        if recovery_date is np.nan:
            output.loc['Recovery Date',col] = recovery_date
            output.loc['Recovery Period',col] = np.nan
        else:
            output.loc['Recovery Date',col] = recovery_date.date()
            output.loc['Recovery Period',col] = (output.loc['Recovery Date',col]\
                                             - output.loc['Min Date',col])\
                                             .days
        
    return output





samples = [('start','2011'),('2012','end'),('start','end')]

_ = stats_over_samples(retsx,samples,stats_mean_vol_sharpe)

_.round(3)








# Stats using excess returns
_ = stats_over_samples(retsx,samples,stats_tail_risk)

_.round(3)


# Stats using total returns
_ = stats_over_samples(rets,samples,stats_tail_risk)

_.round(3)





retsx[['SPY', 'GMWAX']].plot(kind='hist',bins=50,figsize=(12,8),alpha=0.75)
plt.title('Monthly Returns')
plt.show()


(retsx[['SPY', 'GMWAX']] / retsx[['SPY', 'GMWAX']].std()).plot(kind='hist',bins=50,figsize=(12,8),alpha=0.75)
plt.title('Monthly Returns (Volatility Scaled)')
plt.show()











dfs = []

for start, end in samples:
        
    if start == 'start':
            
        if end == 'end':
            data_subsample = retsx
        else:
            data_subsample = retsx.loc[:end]
    else:
            
        if end == 'end':
            data_subsample = retsx.loc[start:]
        else:
            data_subsample = retsx.loc[start:end]
    
    y = data_subsample['GMWAX']
    x = sm.add_constant(data_subsample['SPY'])

    model = sm.OLS(y,x,missing='drop').fit()
    df = model.params
    df.loc['r-squared'] = model.rsquared
    
    dfs.append(df)

_ = pd.concat(objs = dfs,axis=1,keys = [f'{start} - {end}' for start,end in samples])

_.transpose().round(4)





_.loc[['const']] * 12








signals = pd.read_excel('gmo_data.xlsx',sheet_name='signals',index_col='date')








y = retsx['SPY']
x = sm.add_constant(signals['SPX DVD YLD']).shift()

model_dp = sm.OLS(y,x,missing='drop').fit()

print(model_dp.summary())





y = retsx['SPY']
x = sm.add_constant(signals['SPX P/E']).shift()

model_ep = sm.OLS(y,x,missing='drop').fit()

print(model_ep.summary())





y = retsx['SPY']
x = sm.add_constant(signals).shift()

model_all = sm.OLS(y,x,missing='drop').fit()

print(model_all.summary())





# Build the return predictions, and align them with the right period
all_prediction = model_all.predict(sm.add_constant(signals)).shift()
dp_prediction = model_dp.predict(sm.add_constant(signals['SPX DVD YLD'])).shift()
ep_prediction = model_ep.predict(sm.add_constant(signals['SPX P/E'])).shift()

# Build the strategy weighting in SPY
all_weight = 100 * all_prediction
dp_weight = 100 * dp_prediction
ep_weight = 100 * ep_prediction

# Compute the strategy returns
all_strat = all_weight * retsx['SPY']
dp_strat = dp_weight * retsx['SPY']
ep_strat = ep_weight * retsx['SPY']


_ = pd.concat([all_strat,dp_strat,ep_strat],axis=1,keys = ['All','SPX DVD YLD','SPX P/E'])
stats_mean_vol_sharpe(_)


stats_tail_risk(_)


# Redefine the above function
def LFPM_TS(assets, factors, annualize = 12, name = 'asset', treynor = False, mkt_name = 'MKT'):
    '''Alpha, IR, Treynor are annualized (only the stats output)'''

    if isinstance(assets,pd.Series):
        assets = pd.DataFrame(assets)
        assets.columns = [name]
    
    if isinstance(factors,pd.Series):
        factors = pd.DataFrame(factors)
    
    model_output = pd.DataFrame()
    stats_output = pd.DataFrame()
    residuals_output = pd.DataFrame()
    
    x = sm.add_constant(factors)
    
    for asset in assets.columns:
        
        # Create the model
        y = assets[asset]
        model = sm.OLS(y,x,missing='drop').fit()
        
        # Create the parameters dataframe
        model_output[asset] = model.params
        
        # Create the statistics dataframe
        mu = y.mean() * annualize
        alpha = model.params[0] * annualize
        sig_ep = model.resid.std() * np.sqrt(annualize)
        IR = alpha / sig_ep
        r_squared = model.rsquared
        
        stats_output[asset] = pd.Series(data = [alpha,IR,r_squared],index = ['Alpha','IR','$R^{2}$'])
        
        if treynor:
            mkt = model.params[mkt_name]
            TR = mu / mkt
            
            stats_output.loc['Treynor',asset] = TR
        
        # Create the residuals dataframe
        residuals_output[asset] = model.resid
        
    return model_output, stats_output, residuals_output


LFPM_TS(_,retsx['SPY'])[1]











strats_df = pd.concat([_,retsx],axis = 1)

strats_df.quantile(0.05)





(strats_df.loc['2000':'2011'] + 1).cumprod().plot(figsize=(12,8))
plt.axhline(1,c ='black',alpha=0.75,ls = '--')
plt.show()








# All signals
(dp_prediction.dropna() < 0).sum() / len(dp_prediction.dropna())


# All signals
(ep_prediction.dropna() < 0).sum() / len(ep_prediction.dropna())


# All signals
(all_prediction.dropna() < 0).sum() / len(all_prediction.dropna())











def oos_forecast(signals, asset, t = 60, rolling = False, roll_exp = False, intercept = True):
    
    '''
    Computes an out-of-sample forecast based on expanding regression periods
    
    signals: DataFrame containing the signals (regressors) to be used in each regression
    asset: DataFrame containing the values (returns) of the asset being predicted
    t: The minimum number of periods
    rolling: False if expanding, else enter an integer window
    roll_exp: If using rolling, indicate whether to use expanding up to the minimum periods 
    intercept: Boolean indicating the inclusion of an intercept in the regressions
    '''
    
    n = len(signals)
    
    if intercept:
        signals = sm.add_constant(signals)
    
    if t > n:
        
        raise ValueError('Min. periods (t) greater than number of data points')
    
    output = pd.DataFrame(index = signals.index, columns = ['Actual','Predicted','Null'])
    
    # If expanding
    if not rolling:
        
        for i in range(t,n):

            y = asset.iloc[:i]
            x = signals.iloc[:i].shift()

            if intercept:
                null_pred = y.mean()

            else:
                null_pred = 0

            model = sm.OLS(y,x,missing='drop').fit()

            pred_x = signals.iloc[[i - 1]]
            pred = model.predict(pred_x)[0]

            output.iloc[i]['Actual'] = asset.iloc[i]
            output.iloc[i]['Predicted'] = pred
            output.iloc[i]['Null'] = null_pred
    
    # If rolling
    else:
        
        if rolling > n:
            
            raise ValueError('Rolling window greater than number of data points')
        
        y = asset
        x = signals.shift()
        
        if intercept:
            
            if roll_exp:
                null_pred = y.rolling(window = rolling, min_periods = 0).mean().shift()
            else:
                null_pred = y.rolling(window = rolling).mean().shift()

        else:
            null_pred = 0
        
        # When expanding == True, there is a minimum number of observations
        # Keep ^ in mind
        model = RollingOLS(y,x,window = rolling, expanding = roll_exp).fit()

        output['Actual'] = asset
        output['Predicted'] = (model.params * signals).dropna().sum(axis=1).shift()
        output['Null'] = null_pred
        
        
    return output


def oos_r_squared(data):
    
    '''
    Computes the out-of-sample r squared
    data: DataFrame containing actual, model-predicted, and null-predicted values
    '''
    
    model_error = data['Actual'] - data['Predicted']
    null_error = data['Actual'] - data['Null']
    
    r2_oos = 1 - (model_error ** 2).sum() / (null_error ** 2).sum()
    
    return r2_oos


# Create a trading returns func that takes in a trading implementation func


oos_ep_dp = oos_forecast(signals[['SPX DVD YLD', 'SPX P/E']], retsx['SPY'])

oos_r_squared(oos_ep_dp)





oos_ep_dp.dropna().plot(figsize=(12,8))
plt.show()





ep_dp_oos_weight = 100 * oos_ep_dp['Predicted']

ep_dp_oos_strat = (ep_dp_oos_weight * oos_ep_dp['Actual']).astype(float)


(1 + ep_dp_oos_strat).cumprod().plot(figsize=(12,8))
plt.show()





stats_mean_vol_sharpe(ep_dp_oos_strat)


stats_tail_risk(ep_dp_oos_strat.rename('EP DP OOS').to_frame())


LFPM_TS(ep_dp_oos_strat,retsx['SPY'])[1]





ep_dp_oos_strat.quantile(0.05)











# OOS DP and EP signals
(oos_ep_dp['Predicted'].dropna() < 0).sum() / len(oos_ep_dp['Predicted'].dropna())











from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import LinearRegression





def ml_model_predictions(model_cols, returns, signals, return_col, plots = False):
    
    forecasts_ML = returns.loc[:,[return_col]].expanding().mean().shift(1).dropna() 
    forecasts_ML.columns = ['Expanding Mean']

    score_ML = pd.DataFrame(columns=['Expanding Mean'],index=['score'],data=0)

    methods = ['OLS', 'Tree', 'NN']
    est = dict()

    y = returns.loc[:,[return_col]].iloc[1:].squeeze('columns').ravel()
    X = signals.loc[:,model_cols].shift(1).dropna()

    for method in methods:

        if method == 'OLS':
            est[method] = LinearRegression()
        elif method == 'Tree':
            est[method] = RandomForestRegressor(max_depth=3,random_state=1)
        elif method == 'NN':
            est[method] = MLPRegressor(hidden_layer_sizes=500,random_state=1)

        est[method].fit(X,y)
        forecasts_ML[method] = est[method].predict(X)
        score_ML[method] = est[method].score(X,y)    

    forecasts_ML.dropna(inplace=True)
    wts_ML = 100 * forecasts_ML

    spy_ML, _ = returns.loc[:,[return_col]].iloc[1:].align(forecasts_ML, join='right', axis=0)

    fund_returns_ML = wts_ML * spy_ML.values
    fund_returns_ML.insert(0,'Passive', spy_ML)
    
    if plots:
        fn = X.columns
        fig, axes = plt.subplots(nrows = 1,ncols=1, dpi=500);
        tree.plot_tree(est['Tree'].estimators_[0],feature_names = fn, filled=True)
        if len(model_cols) > 1:
            title_name = '-'.join(str(v) for v in model_cols)
        else:
            title_name = model_cols[0]
        plt.title('Signal - '+title_name, fontsize = 20)

    return fund_returns_ML


ML_forecast_DP = ml_model_predictions(['SPX DVD YLD'], retsx, signals, 'SPY')
ML_forecast_EP = ml_model_predictions(['SPX P/E'], retsx, signals, 'SPY')
ML_forecast_All = ml_model_predictions(['SPX DVD YLD','SPX P/E','TNote 10YR'], retsx, signals, 'SPY')





stats_mean_vol_sharpe(ML_forecast_DP)


stats_tail_risk(ML_forecast_DP)





stats_mean_vol_sharpe(ML_forecast_EP)


stats_tail_risk(ML_forecast_EP)





stats_mean_vol_sharpe(ML_forecast_All)


stats_tail_risk(ML_forecast_All)





def oos_ml_model_predictions(model_cols, returns, signals, return_col, window = 60):


    methods = ['OLS', 'Tree', 'NN']
    est = dict()

    forecasts_MLOOS = pd.DataFrame(columns=methods,index=returns.iloc[1:].index,dtype='float64')


    y = returns.loc[:,[return_col]].iloc[1:].squeeze('columns').ravel()
    Xlag = signals.loc[:,model_cols].shift(1).dropna()
    X = signals.loc[:,model_cols]

    for method in methods:

        for t in returns.iloc[1:].index[window-1:]:
            yt = returns.loc[:,[return_col]].iloc[1:].loc[:t].values.ravel()
            Xlag_t = Xlag.loc[:t,:].values
            x_t = X.loc[t,:].values.reshape(1,-1)

            if method == 'OLS':
                est = LinearRegression()
            elif method == 'Tree':
                est = RandomForestRegressor(max_depth=3,random_state=1)
            elif method == 'NN':
                est = MLPRegressor(hidden_layer_sizes=500,random_state=1)

            est.fit(Xlag_t,yt);
            predval = est.predict(x_t)[0]
            forecasts_MLOOS.loc[t,method] = predval

    forecasts_MLOOS.insert(0,'Mean', returns.loc[:,[return_col]].expanding().mean().shift(1).dropna())

    # prefer to date forecast by date of forecasted value, not date it was calculated
    forecasts_MLOOS = forecasts_MLOOS.shift(1).dropna()


    wts_MLOOS = 100 * forecasts_MLOOS

    spy_MLOOS, _ = returns.loc[:,[return_col]].iloc[1:].align(forecasts_MLOOS, join='right', axis=0)

    fund_returns_MLOOS = wts_MLOOS * spy_MLOOS.values
    fund_returns_MLOOS.insert(0,'Passive', spy_MLOOS)

    sigma_t = fund_returns_MLOOS.rolling(24).std()
    relative_vols = pd.DataFrame(sigma_t[['Passive']].to_numpy() / sigma_t.drop(columns=['Passive']).to_numpy(),columns=sigma_t.drop(columns=['Passive']).columns, index=sigma_t.index)
    wts_t = relative_vols * wts_MLOOS
    fund_returns_MLOOS = wts_t * spy_MLOOS.values
    fund_returns_MLOOS.insert(0,'Passive', spy_MLOOS)

    fund_returns_MLOOS.dropna(inplace=True)

    null = returns.loc[:,[return_col]].expanding(window+1).mean().shift(1).dropna() 
    actual = returns.loc[:,[return_col]].iloc[window+1:]

    forecast_err = pd.DataFrame()
    null_err = pd.DataFrame()
    for col in forecasts_MLOOS.columns:
        forecast_err[col] = forecasts_MLOOS[col] - actual[return_col]
        null_err[col] = null[return_col] - actual[return_col]
        
    oos_r2 = 1-(((forecast_err**2).sum())/(null_err**2).sum()).to_frame('OOS R-Squared')


    return (fund_returns_MLOOS,oos_r2)


ML_oos_forecast_DP = oos_ml_model_predictions(['SPX DVD YLD'], retsx, signals, 'SPY')
ML_oos_forecast_EP = oos_ml_model_predictions(['SPX P/E'], retsx, signals, 'SPY')
ML_oos_forecast_All = oos_ml_model_predictions(['SPX DVD YLD','SPX P/E','TNote 10YR'], retsx, signals, 'SPY')





ML_oos_forecast_DP[1]


stats_mean_vol_sharpe(ML_oos_forecast_DP[0])


stats_tail_risk(ML_oos_forecast_DP[0])





ML_oos_forecast_EP[1]


stats_mean_vol_sharpe(ML_oos_forecast_EP[0])


stats_tail_risk(ML_oos_forecast_EP[0])





ML_oos_forecast_All[1]


stats_mean_vol_sharpe(ML_oos_forecast_All[0])


stats_tail_risk(ML_oos_forecast_All[0])
