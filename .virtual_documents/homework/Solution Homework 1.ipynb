


























import pandas as pd
import numpy as np
import os
import sys
import matplotlib.pyplot as plt
current_dir = os.getcwd()
if current_dir[-8:] == "homework":
    parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))
    os.chdir(parent_dir)
    sys.path.insert(0, parent_dir)
import cmds.portfolio_management_helper as pmh

assets_excess_returns = pmh.read_excel_default("data/multi_asset_etf_data.xlsx", sheet_name="excess returns")
assets_excess_returns.tail()


pmh.calc_summary_statistics(
    assets_excess_returns,
    annual_factor=12,
    provided_excess_returns=True,
    keep_columns=['Annualized Vol', 'Annualized Mean', 'Annualized Sharpe']
)


(
    pmh.calc_summary_statistics(
        assets_excess_returns,
        annual_factor=12,
        provided_excess_returns=True,
        keep_columns=['Annualized Sharpe']
    )
    .sort_values("Annualized Sharpe")
    .reset_index()
    .loc[lambda df: df.index.isin([0, len(df.index)-1])]
    .rename(columns={"index": "Asset"})
    .assign(Label=["Worst Sharpe", "Best Sharpe"])
)








pmh.calc_correlations(assets_excess_returns)





pmh.calc_cummulative_returns(assets_excess_returns[['TIP', 'BWX', 'IEF']])


pmh.calc_summary_statistics(
    assets_excess_returns[['TIP', 'BWX', 'IEF']],
    annual_factor=12,
    provided_excess_returns=True,
    keep_columns=[
        "Annualized Mean", "Annualized Vol", "Annualized Sharpe",
        "Max Drawdown", "Peak", "Bottom", "Correlation",
        "Annualized Historical VaR", "Kurtosis", "Skewness"
    ]
).transpose()





analysis_sharpe_vs_tangent_weights = (
    pmh.calc_summary_statistics(
        assets_excess_returns,
        annual_factor=12,
        provided_excess_returns=True,
        keep_columns=['Tangency Weights', 'Annualized Sharpe']
    )
    .sort_values('Annualized Sharpe', ascending=False)
)
analysis_sharpe_vs_tangent_weights


pmh.calc_tangency_weights(assets_excess_returns, return_graphic=True)


pmh.calc_summary_statistics(
    pmh.calc_tangency_weights(assets_excess_returns, return_port_ret=True),
    annual_factor=12,
    provided_excess_returns=True,
    keep_columns=['Annualized Sharpe', 'Annualized Mean', 'Annualized Vol']
)





TIP_ADJUSTMENT = 0.0012

# Modified TIPS
assets_excess_returns_modified_tips = (
    assets_excess_returns
    .assign(TIP=lambda df: df.TIP + TIP_ADJUSTMENT)
)

# No TIPS
assets_excess_returns_no_tips = assets_excess_returns.drop("TIP", axis=1)

pd.concat([
    pmh.calc_tangency_weights(assets_excess_returns_no_tips, name="No TIPS Tangency"),
    pmh.calc_tangency_weights(assets_excess_returns_modified_tips, name="Mod TIPS Tangency"),
    pmh.calc_tangency_weights(assets_excess_returns)
], axis=1)


analysis_tips_portfolios = pd.concat([
    pmh.calc_tangency_weights(assets_excess_returns_modified_tips, name="Mod TIPS Tangency", return_port_ret=True),
    pmh.calc_tangency_weights(assets_excess_returns_no_tips, name="No TIPS Tangency", return_port_ret=True),
    pmh.calc_tangency_weights(assets_excess_returns, return_port_ret=True)
], axis=1)

pmh.calc_summary_statistics(
    analysis_tips_portfolios,
    annual_factor=12,
    provided_excess_returns=True,
    keep_columns=["Annualized Mean", "Annualized Vol", "Annualized Sharpe"]
)














n_assets = len(assets_excess_returns.columns)
MU_MONTH_TARGET = 0.010

portfolio_equal_weights_not_scaled = pmh.create_portfolio(
    assets_excess_returns,
    weights=[1 / n_assets for _ in range(n_assets)],
    port_name="Equal Weights"
)
portfolio_equal_weights = portfolio_equal_weights_not_scaled * MU_MONTH_TARGET / portfolio_equal_weights_not_scaled.mean()
portfolio_equal_weights


asset_variance_dict = assets_excess_returns.std().map(lambda x: x ** 2).to_dict()
asset_inv_variance_dict = {asset: 1 / variance for asset, variance in asset_variance_dict.items()}
portfolio_risk_parity_not_scaled = pmh.create_portfolio(
    assets_excess_returns,
    weights=asset_inv_variance_dict,
    port_name="Risk Parity"
)
portfolio_risk_parity = portfolio_risk_parity_not_scaled * MU_MONTH_TARGET / portfolio_risk_parity_not_scaled.mean()
portfolio_risk_parity


portfolio_tangency_not_scaled = pmh.calc_tangency_weights(assets_excess_returns, return_port_ret=True)
portfolio_tangency = portfolio_tangency_not_scaled * MU_MONTH_TARGET / portfolio_tangency_not_scaled.mean()
portfolio_tangency


portfolio_regularized_not_scaled = pmh.calc_tangency_weights(assets_excess_returns, return_port_ret=True, cov_mat=.5, name="Regularized")
portfolio_regularized = portfolio_regularized_not_scaled * MU_MONTH_TARGET / portfolio_regularized_not_scaled.mean()
portfolio_regularized


portfolios = pd.concat([
    portfolio_equal_weights,
    portfolio_risk_parity,
    portfolio_regularized,
    portfolio_tangency
], axis=1)
pmh.calc_summary_statistics(
    portfolios,
    provided_excess_returns=True,
    annual_factor=12,
    keep_columns=['Annualized Mean', 'Annualized Vol', 'Annualized Sharpe']
)





pmh.calc_correlations(portfolios)











IN_SAMPLE_END_DATE = "2022-12-31"
OUT_OF_SAMPLE_START_DATE = "2023-01-01"

in_sample_assets_excess_returns = assets_excess_returns.loc[:IN_SAMPLE_END_DATE]
out_of_sample_assets_excess_returns = assets_excess_returns.loc[OUT_OF_SAMPLE_START_DATE:]

in_sample_assets_excess_returns_no_tips = assets_excess_returns_no_tips.loc[:IN_SAMPLE_END_DATE]
out_of_sample_assets_excess_returns_no_tips = assets_excess_returns_no_tips.loc[OUT_OF_SAMPLE_START_DATE:]

in_sample_assets_excess_returns_modified_tips = assets_excess_returns_modified_tips.loc[:IN_SAMPLE_END_DATE]
out_of_sample_assets_excess_returns_modified_tips = assets_excess_returns_modified_tips.loc[OUT_OF_SAMPLE_START_DATE:]


# Regularized
in_sample_weights_regularized = pmh.calc_tangency_weights(in_sample_assets_excess_returns, cov_mat=.5, name="Regularized")

# Tangency
in_sample_weights_tangency = pmh.calc_tangency_weights(in_sample_assets_excess_returns)

# No TIPS Tangency
in_sample_weights_no_tips = pmh.calc_tangency_weights(in_sample_assets_excess_returns_no_tips, name="No TIPS Tangency")

# Modified TIPS Tangency
in_sample_weights_modified_tips = pmh.calc_tangency_weights(in_sample_assets_excess_returns_modified_tips, name="Mod TIPS Tangency")

# Risk Parity
in_sample_asset_variance_dict = in_sample_assets_excess_returns.std().map(lambda x: x ** 2).to_dict()
in_sample_asset_inv_variance_dict = {asset: 1 / variance for asset, variance in in_sample_asset_variance_dict.items()}
in_sample_weights_risk_parity = pd.DataFrame(in_sample_asset_inv_variance_dict, index=["Risk Parity Weights"]).transpose()

# Equal Weights
in_sample_weights_equal = pd.DataFrame(
    data=[[1 / n_assets] for _ in range(n_assets)],
    columns=["Equal Weights"],
    index=in_sample_assets_excess_returns.columns
)

in_sample_weights = (
    pd.concat([
        in_sample_weights_regularized,
        in_sample_weights_tangency,
        in_sample_weights_no_tips,
        in_sample_weights_modified_tips,
        in_sample_weights_risk_parity,
        in_sample_weights_equal
    ], axis=1)
    .fillna(0)
)

in_sample_weights


in_sample_weights_scaled = (
    in_sample_weights
    .apply(lambda weights: weights * MU_MONTH_TARGET / (in_sample_assets_excess_returns @ weights).mean())
)
in_sample_weights_scaled


in_sample_weights_scaled.abs().sum().to_frame("Sum of Absolute Weights")


(   
    pmh.calc_summary_statistics(
        in_sample_assets_excess_returns @ in_sample_weights_scaled,
        annual_factor=12,
        provided_excess_returns=True,
        keep_columns=['Annualized mean', 'Annualized vol', 'Annualized sharpe']
    )
    .sort_values('Annualized Sharpe', ascending=False)
)


(
    pmh.calc_summary_statistics(
        out_of_sample_assets_excess_returns @ in_sample_weights_scaled,
        annual_factor=12,
        provided_excess_returns=True,
        keep_columns=['annualized mean', 'annualized vol', 'annualized sharpe']
    )
    .sort_values('Annualized Sharpe', ascending=False)
)








def rolling_oos_performance(
        assets_excess_returns,
        in_sample_end_date,
        out_of_sample_start_date,
        out_of_sample_end_date
    ):
    assets_excess_returns = assets_excess_returns.copy()

    in_sample_assets_excess_returns = assets_excess_returns.loc[:in_sample_end_date]
    out_of_sample_assets_excess_returns = (
        assets_excess_returns.loc[out_of_sample_start_date:out_of_sample_end_date]
    )

    # Regularized
    in_sample_weights_regularized = pmh.calc_tangency_weights(in_sample_assets_excess_returns, cov_mat=.5, name="Regularized")

    # Tangency
    in_sample_weights_tangency = pmh.calc_tangency_weights(in_sample_assets_excess_returns)

    # No TIPS Tangency
    in_sample_weights_no_tips = pmh.calc_tangency_weights(in_sample_assets_excess_returns_no_tips, name="No TIPS Tangency")

    # Modified TIPS Tangency
    in_sample_weights_modified_tips = pmh.calc_tangency_weights(in_sample_assets_excess_returns_modified_tips, name="Mod TIPS Tangency")

    # Risk Parity
    in_sample_asset_variance_dict = in_sample_assets_excess_returns.std().map(lambda x: x ** 2).to_dict()
    in_sample_asset_inv_variance_dict = {asset: 1 / variance for asset, variance in in_sample_asset_variance_dict.items()}
    in_sample_weights_risk_parity = pd.DataFrame(in_sample_asset_inv_variance_dict, index=["Risk Parity Weights"]).transpose()

    # Equal Weights
    in_sample_weights_equal = pd.DataFrame(
        data=[[1 / n_assets] for _ in range(n_assets)],
        columns=["Equal Weights"],
        index=in_sample_assets_excess_returns.columns
    )

    # Join Weights
    in_sample_weights = (
        pd.concat([
            in_sample_weights_regularized,
            in_sample_weights_tangency,
            in_sample_weights_no_tips,
            in_sample_weights_modified_tips,
            in_sample_weights_risk_parity,
            in_sample_weights_equal
        ], axis=1)
        .fillna(0)
    )

    # Rescale Weights
    in_sample_weights_scaled = (
        in_sample_weights
        .apply(lambda weights: weights * MU_MONTH_TARGET / (in_sample_assets_excess_returns @ weights).mean())
    )

    return out_of_sample_assets_excess_returns @ in_sample_weights_scaled


oos_portfolios_performance = pd.DataFrame({})

for in_sample_last_year in range(2015, 2024):
    oos_portfolios_yearly_performance = rolling_oos_performance(
        assets_excess_returns,
        f"{in_sample_last_year}-12-31",
        f"{in_sample_last_year+1}-01-01",
        f"{in_sample_last_year+1}-12-31"
    )
    oos_portfolios_performance = pd.concat([oos_portfolios_performance, oos_portfolios_yearly_performance])

oos_portfolios_performance


pmh.calc_cummulative_returns(oos_portfolios_performance)


pmh.calc_summary_statistics(
    oos_portfolios_performance,
    annual_factor=12,
    provided_excess_returns=True,
    keep_columns=['Annualized Mean', 'Annualized Vol', 'Annualized Sharpe']
)











assets_total_returns = pmh.read_excel_default("data/multi_asset_etf_data.xlsx", sheet_name="total returns")
assets_total_returns.drop("SHV", axis=1, inplace=True)

minimum_variance_weights = pmh.calc_gmv_weights(assets_total_returns)
tangency_weights = pmh.calc_tangency_weights(assets_total_returns)

total_returns_weights = pd.concat([tangency_weights, minimum_variance_weights], axis=1)
total_returns_weights


total_return_portfolios = pd.concat([
    pmh.calc_gmv_weights(assets_total_returns, return_port_ret=True),
    pmh.calc_tangency_weights(assets_total_returns, return_port_ret=True)
], axis=1)

(
    pmh.calc_summary_statistics(
        total_return_portfolios,
        annual_factor=12,
        keep_columns=["Mean", "Vol", "Sharpe"]
    )
    .rename({"Sharpe": "Mean / Vol", "Annualized Sharpe": "Annualized Mean / Vol"}, axis=1)
    .transpose()
)


total_return_portfolios





r_v = total_return_portfolios["GMV Portfolio"].mean()
r_p = MU_MONTH_TARGET
r_t = total_return_portfolios["Tangency Portfolio"].mean()
omega = (r_p - r_v) / (r_t - r_v)
print(f"Omega: {omega:.4f}")


(
    total_returns_weights
    .assign(
        target_return_weights=lambda df: (
            df["Tangency Weights"] * omega
            + df["GMV Weights"] * (1 - omega)
        )
    )
    .rename({"target_return_weights": "Target Return Weights"}, axis=1)
)


pmh.calc_target_ret_weights(MU_MONTH_TARGET, assets_total_returns)


def calc_efficient_frontier(returns, omega_range=[-0.5, 2], return_graphic=False, figsize=(8, 5), annual_factor = 12):
    omega_range = [min(omega_range[0], 0), max(omega_range[1], 1)]
    gmv_returns = pmh.calc_gmv_weights(returns, return_port_ret=True).squeeze()
    tangency_returns = pmh.calc_tangency_weights(returns, return_port_ret=True).squeeze()
    omega_to_efficient_frontier_dict = {}

    for omega in range(int(omega_range[0] * 100), int(omega_range[1] * 100) + 1, 1):
        target_returns = tangency_returns.values * omega / 1e2 + gmv_returns.values * (1 - omega / 1e2)
        omega_to_efficient_frontier_dict[f"{(omega / 1e2):.2f}"] = [target_returns.mean() * annual_factor, target_returns.std() * np.sqrt(annual_factor)]

    efficient_frontier = pd.DataFrame(omega_to_efficient_frontier_dict, index=["Return", "Volatility"]).transpose()

    if not return_graphic:
        return efficient_frontier
    
    plt.figure(figsize=figsize)
    plt.axhline(y=0, linestyle='--', color='black', alpha=0.5)
    plt.plot(
        efficient_frontier['Volatility'],
        efficient_frontier['Return'],
        linestyle='-', marker='', color="black", label="Frontier"
    )
    plt.plot(
        efficient_frontier.loc["1.00", "Volatility"],
        efficient_frontier.loc["1.00", 'Return'], 
        linestyle='', marker='o', color="red", label="Tangency"
    )
    plt.plot(
        efficient_frontier.loc["0.00", "Volatility"],
        efficient_frontier.loc["0.00", 'Return'],
        linestyle='', marker='o', color="blue", label="GMV"
    )

    plt.plot(
        returns.std() * np.sqrt(annual_factor),
        returns.mean() * annual_factor, 
        linestyle='', marker='o', color="purple", label="Assets"
    )
    plt.legend()
    plt.tight_layout()
    plt.show()

calc_efficient_frontier(assets_total_returns, return_graphic=True)



